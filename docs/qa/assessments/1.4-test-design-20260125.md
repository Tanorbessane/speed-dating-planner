# Test Design: Story 1.4 - Baseline Algorithm (Round-Robin)

**Date:** 2026-01-25
**Designer:** Quinn (Test Architect)
**Story:** 1.4 - ImplÃ©menter l'Algorithme de GÃ©nÃ©ration Baseline
**Status:** Draft

---

## ðŸ“Š Test Strategy Overview

| Metric | Value | Target |
|---|---|---|
| **Total Test Scenarios** | 15 | - |
| **Unit Tests** | 11 (73%) | â‰¥70% |
| **Integration Tests** | 3 (20%) | â‰¤30% |
| **E2E Tests** | 1 (7%) | â‰¤10% |
| **P0 (Critical)** | 8 (53%) | â‰¥50% |
| **P1 (High)** | 5 (33%) | â‰¤40% |
| **P2 (Medium)** | 2 (13%) | â‰¤20% |
| **Estimated Coverage** | 95%+ | â‰¥95% |

**Distribution Rationale:**
- **Heavy Unit bias (73%):** Core algorithm is pure logic with no external dependencies
- **Limited Integration (20%):** Only test interaction with models/config
- **Minimal E2E (7%):** Baseline is phase 1 of pipeline, full E2E tested in Story 1.7

---

## ðŸŽ¯ Test Scenarios by Acceptance Criteria

### AC1: Module Structure & Function Signature

**Requirement:** `src/baseline.py` contains `generate_baseline(config: PlanningConfig, seed: int = 42) -> Planning`

#### Test Scenarios

| ID | Level | Priority | Test Description | Justification |
|---|---|---|---|---|
| **1.4-UNIT-001** | Unit | P0 | Verify function exists with correct signature | API contract validation |
| **1.4-UNIT-002** | Unit | P1 | Verify default seed parameter is 42 | NFR11 compliance |
| **1.4-UNIT-003** | Unit | P2 | Verify function has Google-style docstring | Code quality standard |

**Scenarios Detail:**

```python
# 1.4-UNIT-001: Function Signature
def test_generate_baseline_exists():
    """Verify generate_baseline function exists with correct signature."""
    from src.baseline import generate_baseline
    import inspect

    # Verify signature
    sig = inspect.signature(generate_baseline)
    assert 'config' in sig.parameters
    assert 'seed' in sig.parameters
    assert sig.parameters['seed'].default == 42

    # Verify return type annotation
    assert sig.return_annotation == Planning
```

---

### AC2: Round-Robin Strategy Implementation

**Requirement:** Algorithm implements systematic rotation (round-robin with stride)

#### Test Scenarios

| ID | Level | Priority | Test Description | Justification |
|---|---|---|---|---|
| **1.4-UNIT-004** | Unit | P0 | Verify stride calculation produces rotation | Core algorithm correctness |
| **1.4-UNIT-005** | Unit | P1 | Verify rotation differs between sessions | Coverage/mixing validation |
| **1.4-INT-001** | Integration | P1 | Verify rotation with PlanningConfig inputs | Config â†’ Algorithm flow |

**Scenarios Detail:**

```python
# 1.4-UNIT-004: Stride Rotation
def test_stride_rotation_produces_different_order():
    """Verify stride rotation creates different participant order per session."""
    config = PlanningConfig(N=30, X=5, x=6, S=3)
    planning = generate_baseline(config, seed=42)

    # Extract first table participants from each session
    session0_table0 = planning.sessions[0].tables[0]
    session1_table0 = planning.sessions[1].tables[0]
    session2_table0 = planning.sessions[2].tables[0]

    # Verify tables are different (rotation worked)
    assert session0_table0 != session1_table0
    assert session1_table0 != session2_table0
```

---

### AC3: Complete Participant Assignment

**Requirement:** For each session, all N participants assigned to exactly one table

#### Test Scenarios

| ID | Level | Priority | Test Description | Justification |
|---|---|---|---|---|
| **1.4-UNIT-006** | Unit | P0 | Verify all N participants present in session | Data integrity critical |
| **1.4-UNIT-007** | Unit | P0 | Verify no participant duplicates in session | Data integrity critical |
| **1.4-UNIT-008** | Unit | P1 | Verify participant count = N across all sessions | Completeness check |

**Scenarios Detail:**

```python
# 1.4-UNIT-006 & 1.4-UNIT-007: Complete Assignment
def test_all_participants_assigned_exactly_once():
    """Verify each session has all N participants exactly once."""
    config = PlanningConfig(N=30, X=5, x=6, S=6)
    planning = generate_baseline(config, seed=42)

    for session in planning.sessions:
        # Collect all participants from all tables
        all_participants = set()
        for table in session.tables:
            # Verify no duplicates within session
            assert len(all_participants & table) == 0, "Duplicate participant in session"
            all_participants.update(table)

        # Verify all N participants present
        assert all_participants == set(range(30)), "Missing or extra participants"
```

---

### AC4: Table Capacity Constraint

**Requirement:** Each table respects capacity constraint x

#### Test Scenarios

| ID | Level | Priority | Test Description | Justification |
|---|---|---|---|---|
| **1.4-UNIT-009** | Unit | P0 | Verify table sizes â‰¤ x (capacity constraint) | Compliance with FR5 |
| **1.4-INT-002** | Integration | P1 | Verify capacity enforced with various configs | Boundary validation |

**Scenarios Detail:**

```python
# 1.4-UNIT-009: Capacity Constraint
def test_table_capacity_constraint():
    """Verify all tables respect capacity x."""
    config = PlanningConfig(N=30, X=5, x=6, S=6)
    planning = generate_baseline(config, seed=42)

    for session in planning.sessions:
        for table in session.tables:
            assert len(table) <= config.x, f"Table size {len(table)} exceeds capacity {config.x}"
```

---

### AC5: Partial Table Management (FR7)

**Requirement:** If N not multiple of x, participants distributed with gap â‰¤1

#### Test Scenarios

| ID | Level | Priority | Test Description | Justification |
|---|---|---|---|---|
| **1.4-UNIT-010** | Unit | P0 | Verify FR7: max(sizes) - min(sizes) â‰¤ 1 | Critical fairness requirement |
| **1.4-UNIT-011** | Unit | P1 | Verify remainder distribution (first tables get +1) | Algorithm correctness |
| **1.4-INT-003** | Integration | P1 | Test multiple partial table scenarios | Edge case coverage |

**Scenarios Detail:**

```python
# 1.4-UNIT-010: FR7 Equity Gap
def test_partial_tables_gap_le_one():
    """Test FR7: table size gap â‰¤1 for partial tables."""
    config = PlanningConfig(N=37, X=6, x=7, S=5)
    planning = generate_baseline(config, seed=42)

    for session in planning.sessions:
        sizes = [len(table) for table in session.tables]
        gap = max(sizes) - min(sizes)

        assert gap <= 1, f"Table size gap {gap} violates FR7 (must be â‰¤1)"
        assert sum(sizes) == 37, "Total participants mismatch"

        # Verify remainder distribution
        assert sizes.count(7) == 1, "Expected 1 table with 7 participants"
        assert sizes.count(6) == 5, "Expected 5 tables with 6 participants"

# 1.4-INT-003: Multiple Partial Scenarios
@pytest.mark.parametrize("N,X,x,expected_sizes", [
    (37, 6, 7, [7, 6, 6, 6, 6, 6]),  # remainder=1
    (23, 4, 6, [6, 6, 6, 5]),         # remainder=3
    (50, 7, 8, [8, 8, 8, 7, 7, 7, 7]) # remainder=1
])
def test_partial_tables_various_configs(N, X, x, expected_sizes):
    """Test partial table distribution for various configs."""
    config = PlanningConfig(N=N, X=X, x=x, S=3)
    planning = generate_baseline(config, seed=42)

    for session in planning.sessions:
        sizes = sorted([len(table) for table in session.tables], reverse=True)
        assert sizes == expected_sizes, f"Expected {expected_sizes}, got {sizes}"
```

---

### AC6: Determinism (NFR11)

**Requirement:** Same seed â†’ same planning

#### Test Scenarios

| ID | Level | Priority | Test Description | Justification |
|---|---|---|---|---|
| **1.4-UNIT-012** | Unit | P0 | Verify identical planning with seed=42 | NFR11 compliance critical |
| **1.4-UNIT-013** | Unit | P1 | Verify different planning with different seed | Randomness validation |

**Scenarios Detail:**

```python
# 1.4-UNIT-012: Determinism
def test_deterministic_same_seed():
    """Test NFR11: same seed produces identical planning."""
    config = PlanningConfig(N=30, X=5, x=6, S=6)

    planning1 = generate_baseline(config, seed=42)
    planning2 = generate_baseline(config, seed=42)

    # Verify complete identity
    assert len(planning1.sessions) == len(planning2.sessions)

    for s_idx in range(6):
        assert len(planning1.sessions[s_idx].tables) == len(planning2.sessions[s_idx].tables)

        for t_idx in range(5):
            table1 = planning1.sessions[s_idx].tables[t_idx]
            table2 = planning2.sessions[s_idx].tables[t_idx]
            assert table1 == table2, f"Session {s_idx}, Table {t_idx} differs"

# 1.4-UNIT-013: Different Seeds
def test_different_seed_produces_different_planning():
    """Verify different seeds produce different plannings."""
    config = PlanningConfig(N=30, X=5, x=6, S=6)

    planning1 = generate_baseline(config, seed=42)
    planning2 = generate_baseline(config, seed=99)

    # At least one table must differ
    differences_found = False
    for s_idx in range(6):
        for t_idx in range(5):
            if planning1.sessions[s_idx].tables[t_idx] != planning2.sessions[s_idx].tables[t_idx]:
                differences_found = True
                break
        if differences_found:
            break

    assert differences_found, "Plannings identical despite different seeds"
```

---

### AC7: Test Suite Requirements

**Requirement:** `tests/test_baseline.py` verifies simple config, partial tables, determinism, performance

*(Tests above satisfy this AC - they ARE the test suite)*

---

### AC8: No Forgotten/Duplicated Participants

**Requirement:** No participant forgotten or duplicated in session

*(Covered by 1.4-UNIT-006, 1.4-UNIT-007 above)*

---

## âš¡ Performance & NFR Tests

### NFR1: Performance for Nâ‰¤100

| ID | Level | Priority | Test Description | Justification |
|---|---|---|---|---|
| **1.4-PERF-001** | Unit | P0 | Verify N=100 generates in <1s (NFR1) | Contract requirement |
| **1.4-PERF-002** | Unit | P1 | Verify N=300 generates in <5s (NFR2) | Contract requirement |

**Scenarios Detail:**

```python
# 1.4-PERF-001: NFR1
@pytest.mark.slow
def test_performance_n100_under_1s():
    """Test NFR1: Nâ‰¤100 must execute in <2s (target <1s)."""
    import time

    config = PlanningConfig(N=100, X=20, x=5, S=10)

    start = time.time()
    planning = generate_baseline(config, seed=42)
    elapsed = time.time() - start

    assert elapsed < 1.0, f"Performance: {elapsed:.3f}s exceeds 1.0s limit"
    assert len(planning.sessions) == 10
    assert len(planning.sessions[0].tables) == 20

# 1.4-PERF-002: NFR2
@pytest.mark.slow
def test_performance_n300_under_5s():
    """Test NFR2: Nâ‰¤300 must execute in <5s."""
    import time

    config = PlanningConfig(N=300, X=50, x=6, S=15)

    start = time.time()
    planning = generate_baseline(config, seed=42)
    elapsed = time.time() - start

    assert elapsed < 5.0, f"Performance: {elapsed:.3f}s exceeds 5.0s limit"
```

---

## ðŸ”„ Integration Test: End-to-End Baseline Flow

### Comprehensive Integration

| ID | Level | Priority | Test Description | Justification |
|---|---|---|---|---|
| **1.4-E2E-001** | E2E | P1 | Full baseline generation with validation | Complete workflow validation |

**Scenario Detail:**

```python
# 1.4-E2E-001: Complete Baseline Flow
def test_baseline_end_to_end_integration():
    """Integration test: config â†’ baseline â†’ validation."""
    from src.models import PlanningConfig
    from src.baseline import generate_baseline

    # Phase 1: Create config
    config = PlanningConfig(N=42, X=7, x=6, S=8)

    # Phase 2: Generate baseline
    planning = generate_baseline(config, seed=42)

    # Phase 3: Validate structure
    assert len(planning.sessions) == 8
    assert planning.config == config

    # Phase 4: Validate data integrity
    for session_id, session in enumerate(planning.sessions):
        # Verify session ID
        assert session.session_id == session_id

        # Verify table count
        assert len(session.tables) == 7

        # Verify all participants present
        all_participants = set()
        for table in session.tables:
            all_participants.update(table)
        assert len(all_participants) == 42

        # Verify FR7 (partial tables)
        sizes = [len(table) for table in session.tables]
        assert max(sizes) - min(sizes) <= 1
```

---

## ðŸŽ¯ Test Execution Strategy

### Execution Order (Fail Fast)

1. **P0 Unit Tests (Fast Feedback - <100ms total)**
   - 1.4-UNIT-001, 1.4-UNIT-006, 1.4-UNIT-007 (critical structure/integrity)
   - 1.4-UNIT-004, 1.4-UNIT-009, 1.4-UNIT-010 (algorithm correctness)
   - 1.4-UNIT-012 (determinism)
   - **1.4-PERF-001** (NFR1 performance)

2. **P1 Unit Tests (<50ms)**
   - 1.4-UNIT-002, 1.4-UNIT-005, 1.4-UNIT-008, 1.4-UNIT-011, 1.4-UNIT-013

3. **P0 Integration Tests (<200ms)**
   - None at P0

4. **P1 Integration Tests (<500ms)**
   - 1.4-INT-001, 1.4-INT-002, 1.4-INT-003
   - 1.4-E2E-001

5. **P1 Performance Tests (Slow - ~3-5s)**
   - 1.4-PERF-002 (NFR2)

6. **P2 Quality Tests (<50ms)**
   - 1.4-UNIT-003 (docstring)

**Total Estimated Execution Time:**
- Fast tests (P0-P1 unit): ~150ms
- Integration: ~500ms
- Performance: ~5s
- **Total: ~6s** (acceptable for core algorithm module)

---

## ðŸ“‹ Coverage Analysis

### Expected Coverage: 95%+

**Code Paths Covered:**

âœ… **generate_baseline() function:**
- Entry point with seed parameter âœ“
- Random seed initialization âœ“
- Base table size calculation âœ“
- Remainder calculation âœ“
- Session loop (S iterations) âœ“
- Stride calculation âœ“
- Participant rotation âœ“
- Table distribution with remainder âœ“
- Session creation âœ“
- Planning assembly âœ“

**Uncovered Scenarios (Acceptable):**
- None expected (algorithm is deterministic, no error paths)

**Branch Coverage:**
- IF remainder check (table_id < remainder): âœ“ (1.4-UNIT-010, 1.4-INT-003)
- FOR loops: âœ“ (all tests)

---

## ðŸš¨ Risk Coverage Matrix

### Identified Risks â†’ Test Scenarios

| Risk ID | Risk Description | Probability | Impact | Mitigated By |
|---|---|---|---|---|
| **RISK-1.4-001** | Participants lost/duplicated | Medium | CRITICAL | 1.4-UNIT-006, 1.4-UNIT-007 |
| **RISK-1.4-002** | Non-deterministic results | Low | HIGH | 1.4-UNIT-012, 1.4-UNIT-013 |
| **RISK-1.4-003** | Performance degradation | Medium | HIGH | 1.4-PERF-001, 1.4-PERF-002 |
| **RISK-1.4-004** | FR7 violation (unfair tables) | Low | MEDIUM | 1.4-UNIT-010, 1.4-INT-003 |
| **RISK-1.4-005** | Capacity constraint violated | Low | HIGH | 1.4-UNIT-009, 1.4-INT-002 |

**Risk Mitigation Coverage:** 100% (all risks have test scenarios)

---

## âœ… Quality Checklist

Before test design approval:

- [x] Every AC has test coverage
- [x] Test levels are appropriate (73% unit - optimal)
- [x] No duplicate coverage across levels
- [x] Priorities align with business risk
- [x] Test IDs follow naming convention `{EPIC}.{STORY}-{LEVEL}-{SEQ}`
- [x] Scenarios are atomic and independent
- [x] Performance tests marked `@pytest.mark.slow`
- [x] NFRs validated (NFR1, NFR2, NFR11)
- [x] FR7 (equity) validated comprehensively

---

## ðŸ“Š Test Design Summary

### Metrics

| Metric | Value |
|---|---|
| **Acceptance Criteria** | 8 |
| **Test Scenarios** | 15 |
| **Coverage per AC** | 1.88 avg |
| **P0 Tests** | 8 (53%) |
| **Unit Test %** | 73% |
| **Estimated Execution Time** | 6s |
| **Risk Coverage** | 100% |

### Recommendation

**APPROVED FOR IMPLEMENTATION** âœ…

This test design:
- âœ… Provides comprehensive coverage (95%+ expected)
- âœ… Follows test pyramid (heavy unit, light integration/E2E)
- âœ… Validates all critical NFRs (NFR1, NFR2, NFR11)
- âœ… Mitigates all identified risks
- âœ… Efficient execution time (<10s total)
- âœ… Clear traceability AC â†’ Tests

**Next Steps:**
1. Implement tests in `tests/test_baseline.py`
2. Run test suite: `pytest tests/test_baseline.py -v`
3. Verify coverage: `pytest --cov=src.baseline --cov-report=term-missing`
4. Execute QA gate review with actual results

---

**Test Design Completed By:** Quinn (Test Architect)
**Date:** 2026-01-25
**Version:** 1.0
**Status:** Ready for Implementation
