# Story 1.7: Tests d'Intégration Pipeline Baseline Complet

## Status

**Draft**

---

## Story

**As a** développeur,
**I want** des tests d'intégration validant le pipeline complet de génération baseline + métriques,
**so that** je garantis que toutes les pièces fonctionnent ensemble correctement.

---

## Acceptance Criteria

1. Un fichier `tests/test_integration_baseline.py` contient des tests end-to-end
2. Test "Exemple A" (N=30, X=5, x=6, S=6) :
   - Validation config passe
   - Génération baseline produit planning valide
   - Métriques calculées sans erreur
   - Vérification : tous les participants présents chaque session, tables équilibrées
3. Test "Exemple B" (N=100, X=20, x=5, S=10) :
   - Pipeline complet réussit
   - Performance : <2s total (NFR1)
4. Test "Tables partielles" (N=37, X=6, x=7) :
   - Planning généré avec gestion correcte du remainder
   - Écart de taille ≤1 vérifié dans métriques
5. Test "Configuration invalide" :
   - X × x < N → exception levée avec message français
6. Tous les tests d'intégration passent et la suite s'exécute en <5s

---

## Tasks / Subtasks

- [ ] **Task 1: Créer fichier test_integration_baseline.py** (AC: 1)
  - [ ] Créer `tests/test_integration_baseline.py`
  - [ ] Importer tous modules: models, validation, baseline, metrics
  - [ ] Importer pytest, time

- [ ] **Task 2: Implémenter test Exemple A (N=30)** (AC: 2)
  - [ ] Créer config (N=30, X=5, x=6, S=6)
  - [ ] Appeler validate_config(config) - vérifier pas d'exception
  - [ ] Générer planning: planning = generate_baseline(config, seed=42)
  - [ ] Calculer métriques: metrics = compute_metrics(planning, config)
  - [ ] Vérifier planning valide:
    - 6 sessions
    - 5 tables par session
    - Tous participants 0-29 présents chaque session
    - Toutes tables taille 6 (pas de remainder)
  - [ ] Vérifier métriques:
    - total_unique_pairs > 0
    - min_unique ≥ 0, max_unique > 0

- [ ] **Task 3: Implémenter test Exemple B (N=100) avec performance** (AC: 3)
  - [ ] Créer config (N=100, X=20, x=5, S=10)
  - [ ] Mesurer temps pipeline complet:
    - validate_config
    - generate_baseline
    - compute_metrics
  - [ ] Assert temps total < 2.0 secondes (NFR1)
  - [ ] Vérifier planning valide (100 participants, 10 sessions)
  - [ ] Vérifier métriques calculées sans erreur

- [ ] **Task 4: Implémenter test tables partielles (N=37)** (AC: 4)
  - [ ] Créer config (N=37, X=6, x=7, S=5)
  - [ ] Générer planning
  - [ ] Vérifier remainder gestion:
    - remainder = 37 % 6 = 1
    - 1 table taille 7, 5 tables taille 6
  - [ ] Vérifier via métriques:
    - table_sizes_per_session contient {6: 5, 7: 1}
    - Écart taille: max(7) - min(6) = 1 ≤ 1 ✓

- [ ] **Task 5: Implémenter test configuration invalide** (AC: 5)
  - [ ] Créer config invalide (N=50, X=5, x=8, S=3) → 5×8=40<50
  - [ ] Vérifier validate_config lève InvalidConfigurationError
  - [ ] Vérifier message français: "Capacité insuffisante"
  - [ ] Test avec pytest.raises pour vérifier exception et message

- [ ] **Task 6: Implémenter test end-to-end workflow complet**
  - [ ] Test séquence réaliste:
    1. Créer PlanningConfig
    2. Valider avec validate_config
    3. Générer baseline
    4. Calculer métriques
    5. Vérifier invariants (tous participants présents)
  - [ ] Vérifier cohérence entre config et résultats

- [ ] **Task 7: Valider performance suite tests** (AC: 6)
  - [ ] Exécuter `pytest tests/test_integration_baseline.py -v`
  - [ ] Mesurer temps total suite
  - [ ] Assert temps < 5 secondes
  - [ ] Tous tests passent

- [ ] **Task 8: Créer QA Gate YAML**
  - [ ] Créer `docs/qa/gates/1.7-integration-tests.yml`
  - [ ] Checks: pytest integration, timing <5s

---

## Dev Notes

### Dépendance sur Stories Précédentes

**Stories 1.1-1.6 doivent TOUTES être complétées** :
- 1.1: Structure projet + pytest
- 1.2: Models (PlanningConfig, Planning, PlanningMetrics, InvalidConfigurationError)
- 1.3: validate_config
- 1.4: generate_baseline
- 1.5-1.6: compute_metrics (historique + métriques)

Cette story **intègre** tous les modules créés dans Epic 1.

### Tests d'Intégration vs Tests Unitaires

[Source: docs/architecture/stratégie-de-testing.md#5.3]

**Différence clé:**
- **Tests unitaires (Stories 1.2-1.6):** Testent modules isolément
- **Tests d'intégration (cette story):** Testent pipeline complet multi-modules

**Pyramide de tests:**
- 70% unitaires
- 20% intégration ← **cette story**
- 10% e2e (Epic 3 - CLI)

### Exemples de Référence

[Source: docs/prd.md - Epic 1.7]

**Exemple A:** N=30, X=5, x=6, S=6
- Configuration simple, pas de remainder
- Utilisé dans brainstorming session

**Exemple B:** N=100, X=20, x=5, S=10
- Configuration moyenne, test performance NFR1
- Doit s'exécuter en <2s

**Exemple tables partielles:** N=37, X=6, x=7
- Test gestion remainder (FR7)
- Vérifier écart taille ≤1

### Invariants à Vérifier

[Source: docs/architecture/modèles-de-données.md#4.2]

**Invariant 1:** Tous participants assignés exactement 1 fois par session
```python
for session in planning.sessions:
    all_participants = set()
    for table in session.tables:
        all_participants.update(table)
    assert len(all_participants) == config.N
    assert all_participants == set(range(config.N))
```

**Invariant 2:** Capacité tables respectée
```python
for session in planning.sessions:
    for table in session.tables:
        assert len(table) <= config.x
```

**Invariant 3:** Écart taille tables ≤1 (FR7)
```python
for session in planning.sessions:
    sizes = [len(table) for table in session.tables]
    assert max(sizes) - min(sizes) <= 1
```

### Performance NFR1

[Source: docs/prd.md - NFR1]

**NFR1:** "Le système doit générer un planning pour N≤100 participants en moins de 2 secondes"

**Test Exemple B valide NFR1:**
```python
import time

def test_integration_example_b_performance():
    config = PlanningConfig(N=100, X=20, x=5, S=10)

    start = time.time()
    validate_config(config)
    planning = generate_baseline(config, seed=42)
    metrics = compute_metrics(planning, config)
    elapsed = time.time() - start

    assert elapsed < 2.0, f"Pipeline: {elapsed:.3f}s (limite 2s)"
```

---

## Testing

### Exemples de Tests Complets

**1. Test Exemple A - Configuration Simple**
```python
def test_integration_example_a_simple():
    """Test pipeline complet Exemple A (N=30)."""
    # Configuration
    config = PlanningConfig(N=30, X=5, x=6, S=6)

    # Validation
    validate_config(config)  # Ne doit pas lever exception

    # Génération baseline
    planning = generate_baseline(config, seed=42)

    # Vérifications planning
    assert len(planning.sessions) == 6
    for session in planning.sessions:
        assert len(session.tables) == 5
        # Invariant 1: tous participants présents
        all_participants = set()
        for table in session.tables:
            all_participants.update(table)
        assert all_participants == set(range(30))
        # Invariant 2: capacité respectée
        for table in session.tables:
            assert len(table) <= 6
        # Pas de remainder: toutes tables taille 6
        assert all(len(table) == 6 for table in session.tables)

    # Calcul métriques
    metrics = compute_metrics(planning, config)

    # Vérifications métriques
    assert metrics.total_unique_pairs > 0
    assert metrics.min_unique >= 0
    assert metrics.max_unique <= 29  # Max possible: N-1
    assert len(metrics.unique_meetings_per_person) == 30
```

**2. Test Exemple B - Performance NFR1**
```python
import time

def test_integration_example_b_performance():
    """Test pipeline complet Exemple B (N=100) - NFR1."""
    config = PlanningConfig(N=100, X=20, x=5, S=10)

    # Mesurer temps pipeline complet
    start = time.time()

    # Pipeline
    validate_config(config)
    planning = generate_baseline(config, seed=42)
    metrics = compute_metrics(planning, config)

    elapsed = time.time() - start

    # NFR1: <2s pour N≤100
    assert elapsed < 2.0, f"Temps pipeline: {elapsed:.3f}s (limite 2s NFR1)"

    # Vérifier planning valide
    assert len(planning.sessions) == 10
    for session in planning.sessions:
        all_participants = set()
        for table in session.tables:
            all_participants.update(table)
        assert len(all_participants) == 100

    # Vérifier métriques calculées
    assert metrics.total_unique_pairs > 0
```

**3. Test Tables Partielles - FR7**
```python
def test_integration_partial_tables():
    """Test gestion tables partielles (N=37, X=6, x=7)."""
    config = PlanningConfig(N=37, X=6, x=7, S=5)

    validate_config(config)
    planning = generate_baseline(config, seed=42)
    metrics = compute_metrics(planning, config)

    # Vérifier remainder: 37 % 6 = 1
    # → 1 table taille 7, 5 tables taille 6
    for session in planning.sessions:
        sizes = [len(table) for table in session.tables]

        # FR7: écart ≤1
        assert max(sizes) - min(sizes) <= 1

        # Vérifier distribution via métriques
        size_counts = {}
        for table in session.tables:
            size = len(table)
            size_counts[size] = size_counts.get(size, 0) + 1

        assert size_counts[7] == 1  # 1 table taille 7
        assert size_counts[6] == 5  # 5 tables taille 6

    # Vérifier métriques table_sizes_per_session
    for size_counts in metrics.table_sizes_per_session:
        assert size_counts == {6: 5, 7: 1}
```

**4. Test Configuration Invalide**
```python
import pytest

def test_integration_invalid_config():
    """Test configuration invalide lève exception française."""
    # Capacité insuffisante: 5×8=40 < 50
    config = PlanningConfig(N=50, X=5, x=8, S=3)

    with pytest.raises(InvalidConfigurationError,
                      match=r"Capacité insuffisante.*40.*50"):
        validate_config(config)
```

**5. Test End-to-End Workflow Réaliste**
```python
def test_integration_complete_workflow():
    """Test workflow complet de bout en bout."""
    # Étape 1: Créer configuration
    config = PlanningConfig(N=30, X=5, x=6, S=6)

    # Étape 2: Valider
    validate_config(config)

    # Étape 3: Générer planning
    planning = generate_baseline(config, seed=42)

    # Étape 4: Calculer métriques
    metrics = compute_metrics(planning, config)

    # Étape 5: Vérifier cohérence résultats
    assert planning.config == config
    assert len(metrics.unique_meetings_per_person) == config.N

    # Vérifier invariants
    for session in planning.sessions:
        # Tous participants présents
        all_p = set()
        for table in session.tables:
            all_p.update(table)
        assert all_p == set(range(config.N))
```

### Commandes de Test

```bash
# Tests intégration seuls
pytest tests/test_integration_baseline.py -v

# Mesurer temps exécution suite
pytest tests/test_integration_baseline.py -v --durations=0

# Tous tests Epic 1 (unitaires + intégration)
pytest tests/ -v -m "not slow"

# Avec performance
pytest tests/ -v
```

---

## QA Gate

**Fichier:** `docs/qa/gates/1.7-integration-tests.yml`

```yaml
story: 1.7-integration-tests
checks:
  - name: Integration Tests
    command: pytest tests/test_integration_baseline.py -v
    expected: All tests pass

  - name: Suite Timing
    command: pytest tests/test_integration_baseline.py -v --durations=0
    expected: Total time <5s

  - name: Example A Valid
    command: pytest tests/test_integration_baseline.py::test_integration_example_a -v
    expected: Pass

  - name: Example B Performance
    command: pytest tests/test_integration_baseline.py::test_integration_example_b_performance -v
    expected: Pass, <2s (NFR1)

  - name: Partial Tables
    command: pytest tests/test_integration_baseline.py::test_integration_partial_tables -v
    expected: Pass, FR7 validated

  - name: Invalid Config
    command: pytest tests/test_integration_baseline.py::test_integration_invalid_config -v
    expected: Pass, exception raised
```

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-10 | 1.0 | Story initiale créée par Scrum Master | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used

_À compléter par le dev agent_

### Debug Log References

_À compléter par le dev agent_

### Completion Notes List

_À compléter par le dev agent_

### File List

_À compléter par le dev agent_

---

## QA Results

_À compléter par le QA agent_

---

**Definition of Done:**
- [ ] Tous les AC (1-6) sont satisfaits
- [ ] Toutes les tâches et sous-tâches sont complétées
- [ ] `tests/test_integration_baseline.py` créé avec tous tests
- [ ] Test Exemple A (N=30) passe avec tous invariants vérifiés
- [ ] Test Exemple B (N=100) passe en <2s (NFR1 validé)
- [ ] Test tables partielles (N=37) passe avec FR7 vérifié
- [ ] Test config invalide lève exception française
- [ ] Test workflow end-to-end passe
- [ ] Suite complète s'exécute en <5s
- [ ] QA Gate YAML créé dans `docs/qa/gates/1.7-integration-tests.yml`
- [ ] Tous invariants (1-3) vérifiés
- [ ] Cohérence config ↔ planning ↔ metrics validée
- [ ] **Granularité:** 1 story = 1 PR (tests intégration Epic 1)
