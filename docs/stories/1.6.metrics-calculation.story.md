# Story 1.6: Implémenter le Calcul des Métriques de Qualité

## Status

**Draft**

---

## Story

**As a** organisateur d'événement,
**I want** des métriques précises sur la qualité de mon planning,
**so that** je peux évaluer si le planning généré répond à mes besoins d'équité et de diversité.

---

## Acceptance Criteria

1. Une fonction `compute_metrics(planning: Planning, config: PlanningConfig) -> PlanningMetrics` dans `src/metrics.py` calcule toutes les métriques FR8-FR9
2. Les métriques calculées incluent :
   - `total_unique_pairs`: nombre total de paires uniques rencontrées
   - `total_repeat_pairs`: nombre de paires rencontrées plus d'une fois
   - `unique_meetings_per_person`: liste de taille N avec rencontres uniques par participant
   - `min_unique`, `max_unique`, `mean_unique`, `std_unique`: statistiques d'équité
   - `table_sizes_per_session`: distribution des tailles de tables par session
3. La fonction calcule correctement l'écart max-min pour vérifier FR6 (équité ±1)
4. Les tests vérifient le calcul sur des plannings construits manuellement avec résultats attendus connus
5. Les tests vérifient que pour un planning "parfait" (zéro répétition), `total_repeat_pairs == 0`
6. La performance est acceptable : calcul en <100ms pour N=300, S=20
7. La couverture de tests du module `metrics.py` est ≥90%

---

## Tasks / Subtasks

- [ ] **Task 1: Ajouter imports dans metrics.py** (AC: 1)
  - [ ] Importer `statistics` (mean, stdev)
  - [ ] Importer `PlanningMetrics` depuis models
  - [ ] Vérifier compute_meeting_history déjà présent (Story 1.5)

- [ ] **Task 2: Implémenter compute_metrics() - structure** (AC: 1, 2)
  - [ ] Créer fonction `compute_metrics(planning: Planning, config: PlanningConfig) -> PlanningMetrics`
  - [ ] Ajouter docstring Google style complète
  - [ ] Appeler `met_pairs = compute_meeting_history(planning)`
  - [ ] Calculer `total_unique_pairs = len(met_pairs)`

- [ ] **Task 3: Calculer total_repeat_pairs** (AC: 2, 5)
  - [ ] Créer dict `pair_counts: Dict[Tuple[int, int], int]` pour compter occurrences
  - [ ] Parcourir toutes sessions/tables et compter chaque paire
  - [ ] `total_repeat_pairs = sum(1 for count in pair_counts.values() if count > 1)`

- [ ] **Task 4: Calculer unique_meetings_per_person** (AC: 2, 3)
  - [ ] Créer liste `unique_meetings_per_person = [0] * config.N`
  - [ ] Pour chaque paire (p1, p2) dans met_pairs:
    - Incrémenter unique_meetings_per_person[p1]
    - Incrémenter unique_meetings_per_person[p2]

- [ ] **Task 5: Calculer statistiques d'équité** (AC: 2, 3)
  - [ ] `min_unique = min(unique_meetings_per_person)`
  - [ ] `max_unique = max(unique_meetings_per_person)`
  - [ ] `mean_unique = statistics.mean(unique_meetings_per_person)`
  - [ ] `std_unique = statistics.stdev(unique_meetings_per_person) if config.N > 1 else 0.0`

- [ ] **Task 6: Calculer distribution tailles tables** (AC: 2)
  - [ ] Créer `table_sizes_per_session: List[Dict[int, int]] = []`
  - [ ] Pour chaque session:
    - Créer dict `size_counts: Dict[int, int] = {}`
    - Pour chaque table: compter taille
    - Ajouter size_counts à table_sizes_per_session

- [ ] **Task 7: Retourner PlanningMetrics** (AC: 1)
  - [ ] Créer et retourner PlanningMetrics avec tous les champs calculés
  - [ ] Vérifier que equity_gap (@property) fonctionne: max_unique - min_unique

- [ ] **Task 8: Créer tests unitaires** (AC: 4, 5, 7)
  - [ ] Test: planning manuel simple avec résultats connus
    - N=6, 1 session, 2 tables {0,1,2}, {3,4,5}
    - Vérifier total_unique_pairs = 6
    - Vérifier total_repeat_pairs = 0
    - Vérifier unique_meetings_per_person = [2,2,2,2,2,2]
    - Vérifier min=2, max=2, equity_gap=0
  - [ ] Test: planning avec répétitions
    - 2 sessions identiques
    - Vérifier total_repeat_pairs > 0
  - [ ] Test: planning "parfait" zéro répétition
    - Vérifier total_repeat_pairs == 0
  - [ ] Test: tables tailles variables
    - Vérifier table_sizes_per_session correcte

- [ ] **Task 9: Créer test performance** (AC: 6)
  - [ ] Test `@pytest.mark.slow` pour N=300, S=20
  - [ ] Générer planning avec baseline
  - [ ] Mesurer temps compute_metrics
  - [ ] Assert temps < 0.1 seconde (100ms)

- [ ] **Task 10: Valider couverture et qualité** (AC: 7)
  - [ ] `pytest tests/test_metrics.py -v`
  - [ ] Couverture ≥90% pour module metrics.py complet
  - [ ] `mypy src/metrics.py --strict`
  - [ ] `black` et `ruff` passent

- [ ] **Task 11: Créer QA Gate YAML**
  - [ ] Créer `docs/qa/gates/1.6-metrics-calculation.yml`
  - [ ] Checks: pytest, coverage ≥90%, mypy, black, ruff, perf test

---

## Dev Notes

### Dépendance sur Stories Précédentes

**Stories 1.2, 1.4, 1.5 doivent être complétées** :
- 1.2: PlanningConfig, Planning, PlanningMetrics
- 1.4: generate_baseline (pour tests)
- 1.5: compute_meeting_history (utilisée par compute_metrics)

### Implémentation Exacte

[Source: docs/architecture/interfaces-entre-modules.md#3.4]

```python
from typing import Set, Tuple, List, Dict
from statistics import mean, stdev
from src.models import Planning, PlanningConfig, PlanningMetrics

def compute_metrics(planning: Planning, config: PlanningConfig) -> PlanningMetrics:
    """
    Calcule toutes les métriques de qualité d'un planning (FR8-FR9).

    Args:
        planning: Planning à évaluer
        config: Configuration pour contexte

    Returns:
        PlanningMetrics avec toutes les statistiques

    Complexity:
        O(S × X × x²) + O(N) = O(S × X × x²) dominé par historique
    """
    met_pairs = compute_meeting_history(planning)

    # Calcul répétitions (paires rencontrées 2+ fois)
    pair_counts: Dict[Tuple[int, int], int] = {}
    for session in planning.sessions:
        for table in session.tables:
            participants = list(table)
            for i in range(len(participants)):
                for j in range(i + 1, len(participants)):
                    pair = (min(participants[i], participants[j]),
                            max(participants[i], participants[j]))
                    pair_counts[pair] = pair_counts.get(pair, 0) + 1

    total_repeat_pairs = sum(1 for count in pair_counts.values() if count > 1)

    # Calcul rencontres uniques par participant
    unique_meetings_per_person = [0] * config.N
    for p1, p2 in met_pairs:
        unique_meetings_per_person[p1] += 1
        unique_meetings_per_person[p2] += 1

    # Statistiques équité
    min_unique = min(unique_meetings_per_person)
    max_unique = max(unique_meetings_per_person)
    mean_unique = mean(unique_meetings_per_person)
    std_unique = stdev(unique_meetings_per_person) if config.N > 1 else 0.0

    # Distribution tailles de tables par session
    table_sizes_per_session = []
    for session in planning.sessions:
        size_counts: Dict[int, int] = {}
        for table in session.tables:
            size = len(table)
            size_counts[size] = size_counts.get(size, 0) + 1
        table_sizes_per_session.append(size_counts)

    return PlanningMetrics(
        total_unique_pairs=len(met_pairs),
        total_repeat_pairs=total_repeat_pairs,
        unique_meetings_per_person=unique_meetings_per_person,
        min_unique=min_unique,
        max_unique=max_unique,
        mean_unique=mean_unique,
        std_unique=std_unique,
        table_sizes_per_session=table_sizes_per_session
    )
```

### Métriques FR8-FR9

[Source: docs/prd.md - Requirements]

**FR8: Métriques de qualité**
- Nombre total de paires uniques créées → `total_unique_pairs`
- Nombre total de répétitions de paires → `total_repeat_pairs`
- Pour chaque participant : nombre de personnes uniques rencontrées → `unique_meetings_per_person`

**FR9: Statistiques d'équité**
- Minimum, maximum, moyenne et écart-type des rencontres uniques par participant
- Distribution des tailles de tables par session

**FR6 (équité ±1):** Vérifiable via `metrics.equity_gap ≤ 1`

### Complexité Algorithmique

**Temps:** O(S × X × x²)
- Dominé par `compute_meeting_history`: O(S × X × x²)
- Calcul pair_counts: O(S × X × x²) (même complexité)
- Calcul statistiques: O(N)
- Total: O(S × X × x²) car x² domine N généralement

**Mémoire:** O(N² + S×X)
- met_pairs: O(N²) pire cas
- pair_counts: O(N²) pire cas
- unique_meetings_per_person: O(N)
- table_sizes_per_session: O(S×X)

### Cas Limites à Gérer

**N=1:** Pas de paires, stdev undefined
```python
std_unique = stdev(unique_meetings_per_person) if config.N > 1 else 0.0
```

**Planning vide (S=0):** Pas de sessions
- total_unique_pairs = 0
- unique_meetings_per_person = [0] * N
- min_unique = 0, max_unique = 0

**Tables tailles variables:**
- table_sizes_per_session capture correctement distribution

---

## Testing

### Test Coverage Target: ≥90%

[Source: docs/architecture/stratégie-de-testing.md#5.2]

Module metrics.py complet (compute_meeting_history + compute_metrics) doit atteindre 90%+.

### Tests Exhaustifs Requis

**1. Test planning simple résultats connus**
```python
def test_compute_metrics_simple_planning():
    """Test calcul métriques sur planning simple."""
    config = PlanningConfig(N=6, X=2, x=3, S=1)
    session = Session(session_id=0, tables=[{0,1,2}, {3,4,5}])
    planning = Planning(sessions=[session], config=config)

    metrics = compute_metrics(planning, config)

    # 6 paires uniques: (0,1),(0,2),(1,2),(3,4),(3,5),(4,5)
    assert metrics.total_unique_pairs == 6
    assert metrics.total_repeat_pairs == 0
    # Chaque participant rencontre 2 autres
    assert metrics.unique_meetings_per_person == [2,2,2,2,2,2]
    assert metrics.min_unique == 2
    assert metrics.max_unique == 2
    assert metrics.mean_unique == 2.0
    assert metrics.std_unique == 0.0
    assert metrics.equity_gap == 0
```

**2. Test planning avec répétitions**
```python
def test_compute_metrics_with_repeats():
    """Test détection répétitions."""
    config = PlanningConfig(N=4, X=2, x=2, S=2)
    # Session 0 et 1 identiques: {0,1}, {2,3}
    sessions = [
        Session(session_id=0, tables=[{0,1}, {2,3}]),
        Session(session_id=1, tables=[{0,1}, {2,3}])
    ]
    planning = Planning(sessions=sessions, config=config)

    metrics = compute_metrics(planning, config)

    assert metrics.total_unique_pairs == 2  # (0,1), (2,3)
    assert metrics.total_repeat_pairs == 2  # Les 2 paires répétées
    # Chaque participant rencontre 1 autre (2 fois)
    assert metrics.unique_meetings_per_person == [1,1,1,1]
```

**3. Test planning parfait (zéro répétition)**
```python
def test_compute_metrics_perfect_planning():
    """Test planning parfait sans répétitions."""
    config = PlanningConfig(N=30, X=5, x=6, S=6)
    planning = generate_baseline(config, seed=42)

    metrics = compute_metrics(planning, config)

    # Vérifier zéro répétition (idéal)
    # Note: baseline peut avoir répétitions, test conceptuel
    # Pour vraiment tester, créer planning manuel parfait
    assert metrics.total_repeat_pairs >= 0
```

**4. Test distribution tailles tables**
```python
def test_compute_metrics_table_sizes_distribution():
    """Test distribution tailles tables variables."""
    config = PlanningConfig(N=37, X=6, x=7, S=2)
    planning = generate_baseline(config, seed=42)

    metrics = compute_metrics(planning, config)

    # N=37, X=6 → remainder=1
    # Chaque session: 1 table taille 7, 5 tables taille 6
    for size_counts in metrics.table_sizes_per_session:
        assert size_counts[7] == 1  # 1 table taille 7
        assert size_counts[6] == 5  # 5 tables taille 6
```

**5. Test performance N=300**
```python
import time

@pytest.mark.slow
def test_compute_metrics_performance_n300():
    """Test performance: calcul <100ms pour N=300, S=20."""
    config = PlanningConfig(N=300, X=60, x=5, S=20)
    planning = generate_baseline(config, seed=42)

    start = time.time()
    metrics = compute_metrics(planning, config)
    elapsed = time.time() - start

    assert elapsed < 0.1, f"Temps: {elapsed:.3f}s (limite 0.1s)"
    assert metrics.total_unique_pairs > 0
```

### Commandes de Test

```bash
# Tests unitaires (sans slow)
pytest tests/test_metrics.py -v -m "not slow"

# Tests performance
pytest tests/test_metrics.py -v -m "slow"

# Tous tests
pytest tests/test_metrics.py -v

# Couverture module complet
pytest tests/test_metrics.py --cov=src.metrics --cov-report=term --cov-fail-under=90

# Type checking
mypy src/metrics.py --strict

# Qualité code
black src/metrics.py tests/test_metrics.py
ruff check src/metrics.py tests/test_metrics.py
```

---

## QA Gate

**Fichier:** `docs/qa/gates/1.6-metrics-calculation.yml`

```yaml
story: 1.6-metrics-calculation
checks:
  - name: Unit Tests
    command: pytest tests/test_metrics.py -v -m "not slow"
    expected: All tests pass

  - name: Performance Tests
    command: pytest tests/test_metrics.py -v -m "slow"
    expected: All tests pass, N=300 <100ms

  - name: Coverage
    command: pytest tests/test_metrics.py --cov=src.metrics --cov-report=term --cov-fail-under=90
    expected: Coverage ≥90%

  - name: Type Checking
    command: mypy src/metrics.py --strict
    expected: No errors

  - name: Code Formatting
    command: black --check src/metrics.py tests/test_metrics.py
    expected: All files formatted

  - name: Linting
    command: ruff check src/metrics.py tests/test_metrics.py
    expected: No violations
```

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-10 | 1.0 | Story initiale créée par Scrum Master | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used

_À compléter par le dev agent_

### Debug Log References

_À compléter par le dev agent_

### Completion Notes List

_À compléter par le dev agent_

### File List

_À compléter par le dev agent_

---

## QA Results

_À compléter par le QA agent_

---

**Definition of Done:**
- [ ] Tous les AC (1-7) sont satisfaits
- [ ] Toutes les tâches et sous-tâches sont complétées
- [ ] `src/metrics.py` complété avec compute_metrics()
- [ ] `tests/test_metrics.py` complété avec tous tests
- [ ] Tests unitaires passent (pytest -m "not slow")
- [ ] Test performance passe (N=300 <100ms)
- [ ] Couverture ≥90% pour module metrics.py complet
- [ ] `mypy --strict` passe sans erreur
- [ ] `black` et `ruff` passent sans erreur
- [ ] Métriques FR8-FR9 toutes calculées correctement
- [ ] equity_gap (@property) fonctionne
- [ ] Tables tailles variables gérées
- [ ] QA Gate YAML créé dans `docs/qa/gates/1.6-metrics-calculation.yml`
- [ ] Docstring Google style complète
- [ ] **Granularité:** 1 story = 1 PR (complète Story 1.5)
