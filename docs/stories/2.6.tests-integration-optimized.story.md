# Story 2.6: Tests d'Intégration Pipeline Complet Optimisé

## Status

**Draft**

---

## Story

**As a** développeur,
**I want** des tests d'intégration validant que le pipeline complet (3 phases) produit des plannings de haute qualité,
**so that** je garantis la valeur livrée aux utilisateurs finaux.

---

## Acceptance Criteria

1. Un fichier `tests/test_integration_optimized.py` contient des tests end-to-end du pipeline complet
2. Test "Exemple A optimisé" (N=30, X=5, x=6, S=6) :
   - Pipeline 3 phases exécuté
   - Métriques finales : équité ±1 vérifiée, répétitions minimales (<5% du total)
   - Comparaison avant/après : amélioration locale réduit répétitions d'au moins 50% vs baseline
3. Test "Configuration impossible" (N=32, S=3, X=4, x=8) :
   - Détection mathématique : S × (x-1) = 21 < N-1 = 31
   - WARNING loggé indiquant impossibilité
   - Planning généré quand même avec répétitions équitablement réparties
   - Équité ±1 toujours respectée
4. Test "Grande instance" (N=300) :
   - Pipeline complet réussit
   - Performance <5s (NFR2)
   - Équité ±1 garantie
5. Tous les tests d'intégration passent avec couverture ≥85%

---

## Tasks / Subtasks

- [ ] **Task 1: Créer test_integration_optimized.py** (AC: 1)
  - [ ] Créer `tests/test_integration_optimized.py`
  - [ ] Importer: pytest, logging, time, generate_optimized_planning, generate_baseline, compute_metrics

- [ ] **Task 2: Test Exemple A optimisé** (AC: 2)
  - [ ] Config: N=30, X=5, x=6, S=6
  - [ ] Générer baseline seul: `baseline = generate_baseline(config, seed=42)`
  - [ ] Calculer métriques baseline: `metrics_baseline = compute_metrics(baseline, config)`
  - [ ] Générer optimisé: `planning, metrics = generate_optimized_planning(config, seed=42)`
  - [ ] Vérifications:
    - `metrics.equity_gap <= 1`
    - Répétitions < 5% du total paires: `metrics.total_repeat_pairs < metrics.total_unique_pairs * 0.05`
    - Amélioration ≥50% vs baseline: `(metrics_baseline.total_repeat_pairs - metrics.total_repeat_pairs) / metrics_baseline.total_repeat_pairs >= 0.5`

- [ ] **Task 3: Test configuration impossible** (AC: 3)
  - [ ] Config: N=32, S=3, X=4, x=8
  - [ ] Vérifier math: S×(x-1) = 3×7 = 21 < N-1 = 31
  - [ ] Capturer logs WARNING
  - [ ] Appeler generate_optimized_planning
  - [ ] Vérifier planning généré
  - [ ] Vérifier equity_gap ≤ 1 malgré impossibilité

- [ ] **Task 4: Test grande instance N=300** (AC: 4)
  - [ ] Marquer `@pytest.mark.slow`
  - [ ] Config: N=300, X=60, x=5, S=15
  - [ ] Mesurer temps:
    ```python
    start = time.time()
    planning, metrics = generate_optimized_planning(config)
    elapsed = time.time() - start
    ```
  - [ ] Assert: elapsed < 5.0 (NFR2)
  - [ ] Assert: equity_gap ≤ 1

- [ ] **Task 5: Test invariants pipeline complet**
  - [ ] Pour plusieurs configs:
    - Générer planning optimisé
    - Vérifier tous participants présents chaque session
    - Vérifier aucun participant dupliqué
    - Vérifier écart tailles tables ≤1 (FR7)
    - Vérifier equity_gap ≤ 1 (FR6)

- [ ] **Task 6: Test amélioration qualitative**
  - [ ] Comparer baseline vs optimisé
  - [ ] Métriques à comparer:
    - total_repeat_pairs (doit diminuer)
    - std_unique (doit diminuer → plus équitable)
    - equity_gap (doit atteindre ≤1)

- [ ] **Task 7: Valider couverture globale Epic 2** (AC: 5)
  - [ ] `pytest tests/test_integration_optimized.py -v`
  - [ ] `pytest tests/ --cov=src --cov-report=term --cov-fail-under=85`
  - [ ] Vérifier couverture ≥85% pour tous modules Epic 2

- [ ] **Task 8: Créer QA Gate YAML**
  - [ ] Créer `docs/qa/gates/2.6-tests-integration-optimized.yml`

---

## Dev Notes

### Dépendance

**Epic 1 complet + Stories 2.1-2.5 complètes:**
- Pipeline 3 phases fonctionnel
- Benchmarks performance validés

Cette story valide **qualité end-to-end** du pipeline optimisé.

### Métriques Amélioration Attendues

**Exemple A (N=30, X=5, x=6, S=6):**
- Baseline: ~15-20 répétitions
- Optimisé: ~3-7 répétitions (réduction 50-70%)
- Équité: écart ≤1 garanti

**Configuration impossible (N=32, S=3):**
- Répétitions inévitables
- Mais équitablement réparties (tous participants ~même nombre répétitions)
- Équité ±1 respectée

---

## Testing

### Tests Exhaustifs

**1. Test Exemple A optimisé**
```python
def test_integration_example_a_optimized():
    config = PlanningConfig(N=30, X=5, x=6, S=6)

    # Baseline seul
    baseline = generate_baseline(config, seed=42)
    metrics_baseline = compute_metrics(baseline, config)

    # Optimisé
    planning, metrics = generate_optimized_planning(config, seed=42)

    # Vérifications
    assert metrics.equity_gap <= 1
    assert metrics.total_repeat_pairs < metrics.total_unique_pairs * 0.05  # <5% répétitions
    # Amélioration significative
    reduction = (metrics_baseline.total_repeat_pairs - metrics.total_repeat_pairs) / metrics_baseline.total_repeat_pairs
    assert reduction >= 0.5, f"Réduction seulement {reduction*100:.1f}% (attendu ≥50%)"
```

**2. Test configuration impossible**
```python
def test_integration_impossible_configuration(caplog):
    config = PlanningConfig(N=32, X=4, x=8, S=3)

    # Vérifier math
    max_meetings = config.S * (config.x - 1)  # 3 × 7 = 21
    min_needed = config.N - 1  # 31
    assert max_meetings < min_needed  # Configuration impossible

    with caplog.at_level(logging.WARNING):
        planning, metrics = generate_optimized_planning(config)

    # WARNING loggé
    assert any("impossible" in r.message.lower() for r in caplog.records)

    # Planning généré quand même
    assert len(planning.sessions) == 3

    # Équité respectée
    assert metrics.equity_gap <= 1
```

**3. Test grande instance**
```python
@pytest.mark.slow
def test_integration_large_instance_n300():
    config = PlanningConfig(N=300, X=60, x=5, S=15)

    start = time.time()
    planning, metrics = generate_optimized_planning(config)
    elapsed = time.time() - start

    assert elapsed < 5.0, f"NFR2 violé: {elapsed:.3f}s (limite 5s)"
    assert metrics.equity_gap <= 1
```

---

## QA Gate

**Fichier:** `docs/qa/gates/2.6-tests-integration-optimized.yml`

```yaml
story: 2.6-tests-integration-optimized
checks:
  - name: Integration Tests (Fast)
    command: pytest tests/test_integration_optimized.py -v -m "not slow"
    expected: All tests pass

  - name: Integration Tests (Slow)
    command: pytest tests/test_integration_optimized.py -v -m slow
    expected: N=300 <5s

  - name: Example A Optimized
    command: pytest tests/test_integration_optimized.py::test_integration_example_a_optimized -v
    expected: Amélioration ≥50%, équité ±1

  - name: Impossible Config
    command: pytest tests/test_integration_optimized.py::test_integration_impossible_configuration -v
    expected: WARNING logged, planning generated

  - name: Coverage Epic 2
    command: pytest tests/ --cov=src --cov-report=term --cov-fail-under=85
    expected: Coverage ≥85%
```

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-10 | 1.0 | Story initiale créée par Scrum Master | Bob (SM) |

---

**Definition of Done:**
- [ ] `tests/test_integration_optimized.py` créé
- [ ] Test Exemple A: amélioration ≥50%, équité ±1
- [ ] Test config impossible: WARNING loggé, équité ±1
- [ ] Test N=300: performance <5s (NFR2), équité ±1
- [ ] Tous tests intégration passent
- [ ] Couverture globale Epic 2 ≥85%
- [ ] QA Gate YAML créé
- [ ] **Granularité:** 1 story = 1 PR (tests intégration Epic 2)
