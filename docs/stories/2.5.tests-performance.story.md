# Story 2.5: Tests de Performance et Benchmarks

## Status

**Draft**

---

## Story

**As a** développeur,
**I want** des benchmarks systématiques validant les contraintes de performance NFR1-NFR3,
**so that** je garantis que le système reste utilisable en production pour toutes les tailles d'événements.

---

## Acceptance Criteria

1. Un fichier `tests/test_performance.py` contient des benchmarks pour les 3 niveaux de performance
2. Benchmark NFR1 (N=100, X=20, x=5, S=10) :
   - Pipeline complet s'exécute en <2s
   - Résultat enregistré dans fichier JSON (`benchmarks/results.json`)
3. Benchmark NFR2 (N=300, X=60, x=5, S=15) :
   - Pipeline complet s'exécute en <5s
4. Benchmark NFR3 (N=1000, X=200, x=5, S=25) :
   - Pipeline complet s'exécute en <30s
5. Les benchmarks mesurent aussi l'empreinte mémoire (vérification NFR4 : <O(N²))
6. Un script `scripts/run_benchmarks.py` exécute tous les benchmarks et génère un rapport formaté
7. Les tests de performance sont marqués `@pytest.mark.slow` et exclus des tests rapides par défaut
8. Le CI exécute les benchmarks sur chaque merge à `main` et détecte les régressions (>10% ralentissement)

---

## Tasks / Subtasks

- [ ] **Task 1: Créer test_performance.py** (AC: 1, 7)
  - [ ] Créer `tests/test_performance.py`
  - [ ] Importer: pytest, time, json, Path, PlanningConfig, generate_optimized_planning
  - [ ] Marquer tous tests avec `@pytest.mark.slow`

- [ ] **Task 2: Implémenter benchmark NFR1 (N=100 <2s)** (AC: 2, 7)
  - [ ] Test function:
    ```python
    @pytest.mark.slow
    def test_benchmark_nfr1_n100():
        config = PlanningConfig(N=100, X=20, x=5, S=10)
        start = time.time()
        planning, metrics = generate_optimized_planning(config, seed=42)
        elapsed = time.time() - start
        assert elapsed < 2.0, f"NFR1 violé: {elapsed:.3f}s (limite 2s)"
        # Enregistrer résultat
        save_benchmark_result("NFR1", elapsed, config, metrics)
    ```

- [ ] **Task 3: Implémenter benchmark NFR2 (N=300 <5s)** (AC: 3, 7)
  - [ ] Config: N=300, X=60, x=5, S=15
  - [ ] Assert: elapsed < 5.0
  - [ ] Enregistrer résultat

- [ ] **Task 4: Implémenter benchmark NFR3 (N=1000 <30s)** (AC: 4, 7)
  - [ ] Config: N=1000, X=200, x=5, S=25
  - [ ] Assert: elapsed < 30.0
  - [ ] Enregistrer résultat

- [ ] **Task 5: Implémenter mesure empreinte mémoire** (AC: 5)
  - [ ] Utiliser `tracemalloc` ou `memory_profiler`
  - [ ] Mesurer pic mémoire pendant pipeline
  - [ ] Vérifier croissance mémoire ~O(N²) ou mieux
  - [ ] Test helper:
    ```python
    import tracemalloc
    def measure_memory_footprint(config):
        tracemalloc.start()
        planning, metrics = generate_optimized_planning(config)
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        return peak / 1024 / 1024  # MB
    ```

- [ ] **Task 6: Implémenter sauvegarde résultats JSON** (AC: 2)
  - [ ] Créer fonction helper:
    ```python
    def save_benchmark_result(name, elapsed, config, metrics):
        result = {
            "name": name,
            "timestamp": datetime.now().isoformat(),
            "elapsed_seconds": elapsed,
            "config": {"N": config.N, "X": config.X, "x": config.x, "S": config.S},
            "metrics": {
                "total_unique_pairs": metrics.total_unique_pairs,
                "total_repeat_pairs": metrics.total_repeat_pairs,
                "equity_gap": metrics.equity_gap
            }
        }
        # Créer dossier si nécessaire
        Path("benchmarks").mkdir(exist_ok=True)
        # Append to JSON file
        ...
    ```

- [ ] **Task 7: Créer script run_benchmarks.py** (AC: 6)
  - [ ] Créer `scripts/run_benchmarks.py`
  - [ ] Exécuter pytest pour tous tests @pytest.mark.slow
  - [ ] Lire benchmarks/results.json
  - [ ] Générer rapport formaté (console + HTML optionnel)
  - [ ] Afficher tableau comparatif NFR1/NFR2/NFR3

- [ ] **Task 8: Créer configuration CI** (AC: 8)
  - [ ] Créer `.github/workflows/benchmarks.yml` (ou équivalent)
  - [ ] Exécuter benchmarks sur merge main
  - [ ] Comparer avec résultats précédents
  - [ ] Fail si régression >10%
  - [ ] Stocker résultats comme artifact

- [ ] **Task 9: Créer QA Gate YAML**
  - [ ] Créer `docs/qa/gates/2.5-tests-performance.yml`
  - [ ] Checks: pytest -m slow, script run_benchmarks.py

---

## Dev Notes

### Dépendance

**Epic 1 + Stories 2.1-2.4 complètes:**
- Pipeline complet fonctionnel (generate_optimized_planning)

Cette story valide les **NFR1-NFR4** (performance et scalabilité).

### NFR à Valider

[Source: docs/prd.md - NFR1-NFR4]

- **NFR1:** N≤100 en <2s
- **NFR2:** N≤300 en <5s
- **NFR3:** N≤1000 en <30s
- **NFR4:** Empreinte mémoire <O(N²)

### Benchmarking Best Practices

- Seed fixe (seed=42) pour reproductibilité
- Warmup run (ignorer premier run si cache froid)
- Moyenne sur 3 runs pour stabilité (optionnel)
- Enregistrer résultats pour tracking historique

### Format JSON Résultats

```json
{
  "benchmarks": [
    {
      "name": "NFR1",
      "timestamp": "2026-01-10T14:30:00",
      "elapsed_seconds": 1.234,
      "config": {"N": 100, "X": 20, "x": 5, "S": 10},
      "metrics": {
        "total_unique_pairs": 4500,
        "total_repeat_pairs": 50,
        "equity_gap": 1
      },
      "memory_mb": 15.2
    }
  ]
}
```

---

## Testing

### Commandes

```bash
# Tous benchmarks
pytest tests/test_performance.py -v -m slow

# NFR1 uniquement
pytest tests/test_performance.py::test_benchmark_nfr1 -v -m slow

# Script complet
python scripts/run_benchmarks.py
```

---

## QA Gate

**Fichier:** `docs/qa/gates/2.5-tests-performance.yml`

```yaml
story: 2.5-tests-performance
checks:
  - name: Benchmark NFR1
    command: pytest tests/test_performance.py::test_benchmark_nfr1 -v -m slow
    expected: N=100 <2s

  - name: Benchmark NFR2
    command: pytest tests/test_performance.py::test_benchmark_nfr2 -v -m slow
    expected: N=300 <5s

  - name: Benchmark NFR3
    command: pytest tests/test_performance.py::test_benchmark_nfr3 -v -m slow
    expected: N=1000 <30s

  - name: All Benchmarks
    command: pytest tests/test_performance.py -v -m slow
    expected: All pass

  - name: Run Benchmarks Script
    command: python scripts/run_benchmarks.py
    expected: Rapport généré, résultats enregistrés
```

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-10 | 1.0 | Story initiale créée par Scrum Master | Bob (SM) |

---

**Definition of Done:**
- [ ] `tests/test_performance.py` créé avec benchmarks NFR1-NFR3
- [ ] Tous tests marqués `@pytest.mark.slow`
- [ ] NFR1: N=100 <2s validé
- [ ] NFR2: N=300 <5s validé
- [ ] NFR3: N=1000 <30s validé
- [ ] NFR4: empreinte mémoire mesurée
- [ ] Résultats enregistrés dans `benchmarks/results.json`
- [ ] Script `scripts/run_benchmarks.py` créé et fonctionnel
- [ ] Configuration CI créée (détection régressions >10%)
- [ ] QA Gate YAML créé
- [ ] **Granularité:** 1 story = 1 PR (benchmarks performance)
